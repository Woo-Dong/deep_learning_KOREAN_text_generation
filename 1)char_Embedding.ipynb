{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import tensorflow as tf \n",
    "tf.random.set_seed(0) \n",
    "\n",
    "with open('wiki_space_tokenizer.txt', mode='r', encoding='utf-8') as f: \n",
    "    wiki_contents = f.read()\n",
    "wiki_contents = wiki_contents.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1806748"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_contents = wiki_contents[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list() \n",
    "maxlen = 13\n",
    "tmp_content = ''\n",
    "for content in wiki_contents: \n",
    "    content = content.strip()\n",
    "    content = re.sub(r'\\([^)]*\\)', '', content)\n",
    "    content = content.lower()\n",
    "    if not content: continue \n",
    "    if content[-1] == '.':\n",
    "        if len(tmp_content) > 0: tmp_content = tmp_content + '\\n' + content \n",
    "        else: tmp_content = content \n",
    "    else:\n",
    "        if len(tmp_content) > maxlen: \n",
    "            text.append(tmp_content+'E') \n",
    "        tmp_content = ''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110,\n",
       " ['제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령이다.\\n지미 카터는 조지아 주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" 로 알려졌다.\\n1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아 주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\\n1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워, 포드를 누르고 당선되었다.\\n카터 대통령은 에너지 개발을 촉구했으나 공화당의 반대로 무산되었다.\\n카터는 이집트와 이스라엘을 조정하여, 캠프 데이비드에서 안와르 사다트 대통령과 메나헴 베긴 수상과 함께 중동 평화를 위한 캠프데이비드 협정을 체결했다.\\n그러나 이것은 공화당과 미국의 유대인 단체의 반발을 일으켰다. 1979년 백악관에서 양국 간의 평화조약으로 이끌어졌다. 또한 소련과 제2차 전략 무기 제한 협상에 조인했다.\\n카터는 1970년대 후반 당시 대한민국 등 인권 후진국의 국민들의 인권을 지키기 위해 노력했으며, 취임 이후 계속해서 도덕정치를 내세웠다.\\n그러나 주 이란 미국 대사관 인질 사건에서 인질 구출 실패를 이유로 1980년 대통령 선거에서 공화당의 로널드 레이건 후보에게 져 결국 재선에 실패했다. 또한 임기 말기에 터진 소련의 아프가니스탄 침공 사건으로 인해 1980년 하계 올림픽에 반공국가들의 보이콧을 내세웠다.\\n지미 카터는 대한민국과의 관계에서도 중요한 영향을 미쳤던 대통령 중 하나다. 인권 문제와 주한미군 철수 문제로 한때 한미 관계가 불편하기도 했다. 1978년 대한민국에 대한 조선민주주의인민공화국의 위협에 대비해 한미연합사를 창설하면서, 1982년까지 3단계에 걸쳐 주한미군을 철수하기로 했다. 그러나 주한미군사령부와 정보기관·의회의 반대에 부딪혀 주한미군은 완전철수 대신 6,000명을 감축하는 데 그쳤다 . 또한 박정희 정권의 인권 문제 등과의 논란으로 불협화음을 냈으나, 1979년 6월 하순, 대한민국을 방문하여 관계가 다소 회복되었다.\\n1979년 ~ 1980년 대한민국의 정치적 격변기 당시의 대통령이었던 그는 이에 대해 애매한 태도를 보였고, 이는 후에 대한민국 내에서 고조되는 반미 운동의 한 원인이 됐다. 10월 26일, 박정희 대통령이 김재규 중앙정보부장에 의해 살해된 것에 대해 그는 이 사건으로 큰 충격을 받았으며, 사이러스 밴스 국무장관을 조문사절로 파견했다. 12·12 군사 반란과 5.17 쿠데타에 대해 초기에는 강하게 비난했으나, 미국 정부가 신군부를 설득하는데, 한계가 있었고 결국 묵인하는 듯한 태도를 보이게 됐다.\\n퇴임 이후 민간 자원을 적극 활용한 비영리 기구인 카터 재단을 설립한 뒤 민주주의 실현을 위해 제 3세계의 선거 감시 활동 및 기니 벌레에 의한 드라쿤쿠르스 질병 방재를 위해 힘썼다. 미국의 빈곤층 지원 활동, 사랑의 집짓기 운동, 국제 분쟁 중재 등의 활동도 했다.\\n카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 선행하는 행위에 대해 깊은 유감을 표시 하며 미국의 군사적 활동에 강한 반대 입장을 보이고 있다.\\n특히 국제 분쟁 조정을 위해 조선민주주의인민공화국의 김일성, 아이티의 세드라스 장군, 팔레인스타인의 하마스, 보스니아의 세르비아계 정권 같이 미국 정부에 대해 협상을 거부하면서 사태의 위기를 초래한 인물 및 단체를 직접 만나 분쟁의 원인을 근본적으로 해결하기 위해 힘썼다. 이 과정에서 미국 행정부와 갈등을 보이기도 했지만, 전직 대통령의 권한과 재야 유명 인사들의 활약으로 해결해 나갔다.\\n1978년에 채결된 캠프데이비드 협정의 이행이 지지부진 하자 중동 분쟁 분제를 해결하기 위해 1993년 퇴임 후 직접 이스라엘과 팔레인스타인의 오슬로 협정을 이끌어 내는 데도 성공했다.\\n1993년 1차 북핵 위기 당시 조선민주주의인민공화국에 대한 미국의 군사적 행동이 임박했으나, 미국 전직 대통령으로는 처음으로 조선민주주의인민공화국을 방문하고 미국과 조선민주주의인민공화국 양국의 중재에 큰 기여를 해 위기를 해결했다는 평가를 받았다. 또한 이 때 김일성 주석과 김영삼 대통령의 만남을 주선했다. 하지만 그로부터 수주일 후 김일성이 갑자기 사망하여 김일성과 김영삼의 만남은 이루어지지 못했다.\\n미국의 관타나모 수용소 문제, 세계의 인권문제에서도 관심이 깊어 유엔에 유엔인권고등판무관의 제도를 시행하도록 노력하여 독재자들의 인권 유린에 대해 제약을 하고, 국제형사재판소를 만드는 데 기여하여 독재자들 같은 인권유린범죄자를 재판소로 회부하여 국제적인 처벌을 받게 하는 등 인권 신장에 크나 큰 기여를 했다.\\n2011년 4월 26일부터 29일까지 조선민주주의인민공화국을 3일간 방문했다.\\n경제문제를 해결하지 못하고 주 이란 미국 대사관 인질 사건에 발목이 잡혀 실패한 대통령으로 평가를 받지만 이란 사태는 미국 내 이란 재산을 풀어주겠다는 조건을 내세워서 사실상 카터가 해결한 것이었고, 사랑의 집짓기 운동 등으로 퇴임 후에 훨씬 더 존경받는 미국 대통령 중에 특이한 인물로 남았다.\\n그는 2002년 말 인권과 중재 역할에 대한 공로를 인정받아 노벨 평화상을 받게 되었다.E',\n",
       "  '수학은 양, 구조, 공간, 변화 등 개념을 다루는 학문이다. 현대 수학은 형식 논리를 이용해서 공리로 구성된 추상적 구조를 연구하는 학문으로 여겨지기도 한다. 수학은 그 구조와 발전 과정에서는 자연과학에 속하는 물리학을 비롯한 다른 학문들과 깊은 연관을 맺고 있으나, 여느 과학의 분야들과는 달리, 자연계에서 관측되지 않는 개념들에 대해서까지 이론을 일반화 및 추상화시킬 수 있다는 차이가 있다. 수학자들은 그러한 개념들에 대해서 추측을 하고, 적절하게 선택된 정의와 공리로부터의 엄밀한 연역을 통해서 추측들의 진위를 파악한다.\\n수학은 숫자 세기, 계산, 측정 및 물리적 대상의 모양과 움직임을 추상화하고, 이에 논리적 추론을 적용하여 나타났다. 이런 기본 개념들은 고대 이집트, 메소포타미아, 고대 인도, 고대 중국 및 고대 그리스의 수학책에서 찾아볼 수 있으며, 유클리드의 원론에서는 엄밀한 논증이 발견된다. 이런 발전은 그 뒤로도 계속되어, 16세기의 르네상스에 이르러서는 수학적 발전과 과학적 방법들의 상호 작용이 일어나, 혁명적인 연구들이 진행되며 인류 문명에 큰 영향을 미치게 되었고, 이는 현재까지도 계속되고 있다.\\n오늘날 수학은 자연과학, 공학, 의학뿐만 아니라, 경제학 등의 사회과학에서도 중요한 도구로서 사용된다. 수학을 이런 분야들에 적용한 응용수학은 그 결과로서 수학 자체의 발전을 이끌고 새로운 분야들을 낳았다. 응용이 아닌 수학 자체의 아름다움과 재미를 추구하며 연구하는 것을 순수수학이라 하는데, 긴 시간이 지난 뒤에 순수수학적 연구를 다른 분야에 응용할 방법이 발견된 경우도 많았다고 한다.\\n대부분 자료를 보면, \"mathematics\"는 \"수리적인\"이라는 뜻을 가진 라틴어 mathmaticus와 그리스어 mathematikos에서 유래되었거나, \"학식있는\"을 뜻하는 mathema와 \"배우다\"를 뜻하는 manthanein에서 유래되었다고 한다.\\n수학은 기원전 600년 경에 살았던 탈레스로부터 시작됐다. 하지만 탈레스가 태어나기 전에도 수학을 연구한 사람이 있을 수도 있기 때문에, 인류의 역사와 더불어 시작되었다고 할 수 있다. 교역•분배•과세 등의 인류의 사회 생활에 필요한 모든 계산을 수학이 담당해 왔고, 농경 생활에 필수적인 천문 관측과 달력의 제정, 토지의 측량 또한 수학이 직접적으로 관여한 분야이다. 고대 수학을 크게 발전시킨 나라로는 이집트, 인도, 그리스, 중국 등이 있다. 그 중에서도 그리스는 처음으로 수학의 방정식에서 변수를 문자로 쓴 나라이다.\\n한국의 수학은 약 1,500년 전부터 기록으로 보이기 시작한다. 신라 시대에 수학을 가르쳤으며, 탈레스가 최초로 발견한 일식과 월식을 예측할 정도로 발달했다. 조선 시대에 훈민정음을 창제한 세종 대왕은 집현전 학자들에게 수학 연구를 명하는 등, 조선의 수학 수준을 향상시키기 위해서 많은 노력을 기울였다. 하지만 임진왜란으로 많은 서적들이 불타고, 천문학 분야에서 큰 손실을 입었다. 조선 후기의 한국의 수학은 실학자들을 중심으로 다시 발전하였고, 새로운 결과도 성취되었다. 그렇게 해서 한국은 수학이 현재까지 발전하게 되었다.\\n수학의 각 분야들은 상업에 필요한 계산을 하기 위해, 숫자들의 관계를 이해하기 위해, 토지를 측량하기 위해, 그리고 천문학적 사건들을 예견하기 위해 발전되어왔다. 이 네 가지 목적은 대략적으로 수학이 다루는 대상인 양, 구조, 공간 및 변화에 대응되며, 이들을 다루는 수학의 분야를 각각 산술, 대수학, 기하학, 해석학이라 한다. 또한 이 밖에도 근대 이후에 나타난 수학기초론과 이산수학 및 응용수학 등이 있다.\\n산술은 자연수와 정수 및 이에 대한 사칙연산에 대한 연구로서 시작했다. 수론은 이런 주제들을 보다 깊게 다루는 학문으로, 그 결과로는 페르마의 마지막 정리 등이 유명하다. 또한 쌍둥이 소수 추측과 골트바흐 추측 등을 비롯해 오랜 세월 동안 해결되지 않고 남아있는 문제들도 여럿 있다.\\n수의 체계가 보다 발전하면서, 정수의 집합을 유리수의 집합의 부분집합으로 여기게 되었다. 또한 유리수의 집합은 실수의 집합의 부분집합이며, 이는 또다시 복소수 집합의 일부분으로 볼 수 있다. 여기에서 더 나아가면 사원수와 팔원수 등의 개념을 생각할 수도 있다. 이와는 약간 다른 방향으로, 자연수를 무한대까지 세어나간다는 개념을 형식화하여 순서수의 개념을 얻으며, 집합의 크기 비교를 이용하여 무한대를 다루기 위한 또다른 방법으로는 기수의 개념도 있다.\\n수 대신 문자를 써서 문제해결을 쉽게 하는 것과, 마찬가지로 수학적 법칙을 일반적이고 간명하게 나타내는 것을 포함한다. 고전대수학은 대수방정식 및 연립방정식의 해법에서 시작하여 군, 환, 체 등의 추상대수학을 거쳐 현대에 와서는 대수계의 구조를 보는 것을 중심으로 하는 선형대수학으로 전개되었다. 수의 집합이나 함수와 같은 많은 수학적 대상들은 내재적인 구조를 보인다. 이러한 대상들의 구조적 특성들이 군론, 환론, 체론 그리고 그 외의 수많은 대수적 구조들을 연구하면서 다루어지며, 그것들 하나하나가 내재적 구조를 지닌 수학적 대상이다. 이 분야에서 중요한 개념은 벡터, 벡터 공간으로의 일반화, 그리고 선형대수학에서의 지식들이다. 벡터의 연구에는 산술, 대수, 기하라는 수학의 중요한 세개의 분야가 조합되어 있다. 벡터 미적분학은 여기에 해석학의 영역이 추가된다. 텐서 미적분학은 대칭성과 회전축의 영향 아래에서 벡터의 움직임을 연구한다. 눈금없는 자와 컴퍼스와 관련된 많은 고대의 미해결 문제들이 갈루아 이론을 사용하여 비로소 해결되었다.\\n공간에 대한 연구는 기하학에서 시작되었고, 특히 유클리드 기하학에서 비롯되었다. 삼각법은 공간과 수들을 결합하였고, 잘 알려진 피타고라스의 정리를 포함한다. 현대에 와서 공간에 대한 연구는, 이러한 개념들은 더 높은 차원의 기하학을 다루기 위해 비유클리드 기하학과 위상수학으로 일반화되었다. 수론과 공간에 대한 이해는 모두 해석 기하학, 미분기하학, 대수기하학에 중요한 역할을 한다. 리 군도 공간과 구조, 변화를 다루는데 사용된다. 위상수학은 20세기 수학의 다양한 지류속에서 괄목할만한 성장을 한 분야이며, 푸앵카레 추측과 인간에 의해서 증명되지 못하고 오직 컴퓨터로만 증명된 4색정리를 포함한다.\\n변화에 대한 이해와 묘사는 자연과학에 있어서 일반적인 주제이며, 미적분학은 변화를 탐구하는 강력한 도구로서 발전되었다. 함수는 변화하는 양을 묘사함에 있어서 중추적인 개념으로써 떠오르게 된다. 실수와 실변수로 구성된 함수의 엄밀한 탐구가 실해석학이라는 분야로 알려지게 되었고, 복소수에 대한 이와 같은 탐구 분야는 복소해석학이라고 한다. 함수해석학은 함수의 공간의 탐구에 주목한다. 함수해석학의 많은 응용분야 중 하나가 양자역학이다. 많은 문제들이 자연스럽게 양과 그 양의 변화율의 관계로 귀착되고, 이러한 문제들이 미분방정식으로 다루어진다. 자연의 많은 현상들이 동역학계로 기술될 수 있다. 혼돈 이론은 이러한 예측 불가능한 현상을 탐구하는 데 상당한 기여를 한다.\\n수학의 기초를 확실히 세우기 위해, 수리논리학과 집합론이 발전하였고, 이와 더불어 범주론이 최근에도 발전되고 있다. “근본 위기”라는 말은 대략 1900년에서 1930년 사이에 일어난, 수학의 엄밀한 기초에 대한 탐구를 상징적으로 보여주는 말이다. 수학의 엄밀한 기초에 대한 몇 가지 의견 불일치는 오늘날에도 계속되고 있다. 수학의 기초에 대한 위기는 그 당시 수많은 논쟁에 의해 촉발되었으며, 그 논쟁에는 칸토어의 집합론과 브라우어-힐베르트 논쟁이 포함되었다.E'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text), text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142403"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(elem) for elem in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = ''.join(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars:  1109\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(total_text)))\n",
    "char_size = len(chars)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print('total chars: ', char_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences length:  140973\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "next_chars = []\n",
    "for content in text: \n",
    "    for i in range(0, len(content) - maxlen, 1):\n",
    "        sentences.append([char_indices[elem] for elem in content[i: i + maxlen]])\n",
    "        next_chars.append(char_indices[content[i + maxlen]])\n",
    "print(\"sequences length: \", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical \n",
    "import numpy as np\n",
    "data_X = np.array(sentences) \n",
    "data_Y = np.array(next_chars) \n",
    "data_Y = to_categorical(data_Y)\n",
    "y_size = data_Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=char_size, \n",
    "                   output_dim=200, \n",
    "                   input_length=maxlen))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(y_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140973 samples\n",
      "Epoch 1/400\n",
      "140973/140973 - 10s - loss: 4.7611 - accuracy: 0.2245\n",
      "Epoch 2/400\n",
      "140973/140973 - 7s - loss: 4.1474 - accuracy: 0.2643\n",
      "Epoch 3/400\n",
      "140973/140973 - 7s - loss: 3.8411 - accuracy: 0.2888\n",
      "Epoch 4/400\n",
      "140973/140973 - 7s - loss: 3.6653 - accuracy: 0.3048\n",
      "Epoch 5/400\n",
      "140973/140973 - 7s - loss: 3.5396 - accuracy: 0.3187\n",
      "Epoch 6/400\n",
      "140973/140973 - 7s - loss: 3.4373 - accuracy: 0.3314\n",
      "Epoch 7/400\n",
      "140973/140973 - 7s - loss: 3.3499 - accuracy: 0.3420\n",
      "Epoch 8/400\n",
      "140973/140973 - 7s - loss: 3.2738 - accuracy: 0.3528\n",
      "Epoch 9/400\n",
      "140973/140973 - 7s - loss: 3.2057 - accuracy: 0.3635\n",
      "Epoch 10/400\n",
      "140973/140973 - 7s - loss: 3.1454 - accuracy: 0.3701\n",
      "Epoch 11/400\n",
      "140973/140973 - 7s - loss: 3.0924 - accuracy: 0.3767\n",
      "Epoch 12/400\n",
      "140973/140973 - 7s - loss: 3.0461 - accuracy: 0.3810\n",
      "Epoch 13/400\n",
      "140973/140973 - 7s - loss: 3.0042 - accuracy: 0.3859\n",
      "Epoch 14/400\n",
      "140973/140973 - 7s - loss: 2.9664 - accuracy: 0.3903\n",
      "Epoch 15/400\n",
      "140973/140973 - 7s - loss: 2.9323 - accuracy: 0.3938\n",
      "Epoch 16/400\n",
      "140973/140973 - 7s - loss: 2.9016 - accuracy: 0.3979\n",
      "Epoch 17/400\n",
      "140973/140973 - 7s - loss: 2.8723 - accuracy: 0.4007\n",
      "Epoch 18/400\n",
      "140973/140973 - 7s - loss: 2.8450 - accuracy: 0.4051\n",
      "Epoch 19/400\n",
      "140973/140973 - 7s - loss: 2.8199 - accuracy: 0.4083\n",
      "Epoch 20/400\n",
      "140973/140973 - 7s - loss: 2.7969 - accuracy: 0.4112\n",
      "Epoch 21/400\n",
      "140973/140973 - 7s - loss: 2.7741 - accuracy: 0.4143\n",
      "Epoch 22/400\n",
      "140973/140973 - 7s - loss: 2.7531 - accuracy: 0.4172\n",
      "Epoch 23/400\n",
      "140973/140973 - 7s - loss: 2.7328 - accuracy: 0.4203\n",
      "Epoch 24/400\n",
      "140973/140973 - 7s - loss: 2.7136 - accuracy: 0.4229\n",
      "Epoch 25/400\n",
      "140973/140973 - 8s - loss: 2.6955 - accuracy: 0.4262\n",
      "Epoch 26/400\n",
      "140973/140973 - 8s - loss: 2.6775 - accuracy: 0.4279\n",
      "Epoch 27/400\n",
      "140973/140973 - 8s - loss: 2.6607 - accuracy: 0.4312\n",
      "Epoch 28/400\n",
      "140973/140973 - 8s - loss: 2.6443 - accuracy: 0.4331\n",
      "Epoch 29/400\n",
      "140973/140973 - 8s - loss: 2.6284 - accuracy: 0.4359\n",
      "Epoch 30/400\n",
      "140973/140973 - 8s - loss: 2.6138 - accuracy: 0.4380\n",
      "Epoch 31/400\n",
      "140973/140973 - 8s - loss: 2.5990 - accuracy: 0.4406\n",
      "Epoch 32/400\n",
      "140973/140973 - 7s - loss: 2.5844 - accuracy: 0.4431\n",
      "Epoch 33/400\n",
      "140973/140973 - 7s - loss: 2.5703 - accuracy: 0.4443\n",
      "Epoch 34/400\n",
      "140973/140973 - 7s - loss: 2.5576 - accuracy: 0.4466\n",
      "Epoch 35/400\n",
      "140973/140973 - 7s - loss: 2.5439 - accuracy: 0.4486\n",
      "Epoch 36/400\n",
      "140973/140973 - 7s - loss: 2.5313 - accuracy: 0.4510\n",
      "Epoch 37/400\n",
      "140973/140973 - 7s - loss: 2.5189 - accuracy: 0.4530\n",
      "Epoch 38/400\n",
      "140973/140973 - 7s - loss: 2.5063 - accuracy: 0.4554\n",
      "Epoch 39/400\n",
      "140973/140973 - 7s - loss: 2.4950 - accuracy: 0.4566\n",
      "Epoch 40/400\n",
      "140973/140973 - 8s - loss: 2.4838 - accuracy: 0.4580\n",
      "Epoch 41/400\n",
      "140973/140973 - 8s - loss: 2.4720 - accuracy: 0.4604\n",
      "Epoch 42/400\n",
      "140973/140973 - 8s - loss: 2.4607 - accuracy: 0.4630\n",
      "Epoch 43/400\n",
      "140973/140973 - 8s - loss: 2.4498 - accuracy: 0.4642\n",
      "Epoch 44/400\n",
      "140973/140973 - 8s - loss: 2.4392 - accuracy: 0.4670\n",
      "Epoch 45/400\n",
      "140973/140973 - 8s - loss: 2.4289 - accuracy: 0.4686\n",
      "Epoch 46/400\n",
      "140973/140973 - 8s - loss: 2.4181 - accuracy: 0.4698\n",
      "Epoch 47/400\n",
      "140973/140973 - 8s - loss: 2.4090 - accuracy: 0.4710\n",
      "Epoch 48/400\n",
      "140973/140973 - 8s - loss: 2.3988 - accuracy: 0.4728\n",
      "Epoch 49/400\n",
      "140973/140973 - 7s - loss: 2.3889 - accuracy: 0.4748\n",
      "Epoch 50/400\n",
      "140973/140973 - 7s - loss: 2.3795 - accuracy: 0.4768\n",
      "Epoch 51/400\n",
      "140973/140973 - 7s - loss: 2.3705 - accuracy: 0.4779\n",
      "Epoch 52/400\n",
      "140973/140973 - 7s - loss: 2.3613 - accuracy: 0.4796\n",
      "Epoch 53/400\n",
      "140973/140973 - 7s - loss: 2.3525 - accuracy: 0.4812\n",
      "Epoch 54/400\n",
      "140973/140973 - 7s - loss: 2.3437 - accuracy: 0.4827\n",
      "Epoch 55/400\n",
      "140973/140973 - 8s - loss: 2.3343 - accuracy: 0.4842\n",
      "Epoch 56/400\n",
      "140973/140973 - 8s - loss: 2.3259 - accuracy: 0.4857\n",
      "Epoch 57/400\n",
      "140973/140973 - 8s - loss: 2.3174 - accuracy: 0.4879\n",
      "Epoch 58/400\n",
      "140973/140973 - 8s - loss: 2.3095 - accuracy: 0.4895\n",
      "Epoch 59/400\n",
      "140973/140973 - 8s - loss: 2.3013 - accuracy: 0.4906\n",
      "Epoch 60/400\n",
      "140973/140973 - 8s - loss: 2.2931 - accuracy: 0.4918\n",
      "Epoch 61/400\n",
      "140973/140973 - 8s - loss: 2.2853 - accuracy: 0.4928\n",
      "Epoch 62/400\n",
      "140973/140973 - 9s - loss: 2.2774 - accuracy: 0.4949\n",
      "Epoch 63/400\n",
      "140973/140973 - 8s - loss: 2.2687 - accuracy: 0.4967\n",
      "Epoch 64/400\n",
      "140973/140973 - 8s - loss: 2.2629 - accuracy: 0.4977\n",
      "Epoch 65/400\n",
      "140973/140973 - 8s - loss: 2.2547 - accuracy: 0.4997\n",
      "Epoch 66/400\n",
      "140973/140973 - 7s - loss: 2.2478 - accuracy: 0.5007\n",
      "Epoch 67/400\n",
      "140973/140973 - 7s - loss: 2.2400 - accuracy: 0.5025\n",
      "Epoch 68/400\n",
      "140973/140973 - 7s - loss: 2.2328 - accuracy: 0.5038\n",
      "Epoch 69/400\n",
      "140973/140973 - 7s - loss: 2.2264 - accuracy: 0.5052\n",
      "Epoch 70/400\n",
      "140973/140973 - 8s - loss: 2.2195 - accuracy: 0.5055\n",
      "Epoch 71/400\n",
      "140973/140973 - 8s - loss: 2.2129 - accuracy: 0.5065\n",
      "Epoch 72/400\n",
      "140973/140973 - 8s - loss: 2.2054 - accuracy: 0.5092\n",
      "Epoch 73/400\n",
      "140973/140973 - 8s - loss: 2.1990 - accuracy: 0.5102\n",
      "Epoch 74/400\n",
      "140973/140973 - 8s - loss: 2.1922 - accuracy: 0.5113\n",
      "Epoch 75/400\n",
      "140973/140973 - 8s - loss: 2.1864 - accuracy: 0.5128\n",
      "Epoch 76/400\n",
      "140973/140973 - 8s - loss: 2.1793 - accuracy: 0.5140\n",
      "Epoch 77/400\n",
      "140973/140973 - 8s - loss: 2.1738 - accuracy: 0.5150\n",
      "Epoch 78/400\n",
      "140973/140973 - 8s - loss: 2.1679 - accuracy: 0.5161\n",
      "Epoch 79/400\n",
      "140973/140973 - 8s - loss: 2.1624 - accuracy: 0.5180\n",
      "Epoch 80/400\n",
      "140973/140973 - 7s - loss: 2.1559 - accuracy: 0.5190\n",
      "Epoch 81/400\n",
      "140973/140973 - 7s - loss: 2.1492 - accuracy: 0.5202\n",
      "Epoch 82/400\n",
      "140973/140973 - 7s - loss: 2.1438 - accuracy: 0.5209\n",
      "Epoch 83/400\n",
      "140973/140973 - 7s - loss: 2.1386 - accuracy: 0.5214\n",
      "Epoch 84/400\n",
      "140973/140973 - 8s - loss: 2.1323 - accuracy: 0.5231\n",
      "Epoch 85/400\n",
      "140973/140973 - 8s - loss: 2.1270 - accuracy: 0.5238\n",
      "Epoch 86/400\n",
      "140973/140973 - 8s - loss: 2.1223 - accuracy: 0.5246\n",
      "Epoch 87/400\n",
      "140973/140973 - 8s - loss: 2.1168 - accuracy: 0.5258\n",
      "Epoch 88/400\n",
      "140973/140973 - 8s - loss: 2.1103 - accuracy: 0.5273\n",
      "Epoch 89/400\n",
      "140973/140973 - 8s - loss: 2.1054 - accuracy: 0.5281\n",
      "Epoch 90/400\n",
      "140973/140973 - 8s - loss: 2.1004 - accuracy: 0.5288\n",
      "Epoch 91/400\n",
      "140973/140973 - 8s - loss: 2.0976 - accuracy: 0.5289\n",
      "Epoch 92/400\n",
      "140973/140973 - 8s - loss: 2.0908 - accuracy: 0.5321\n",
      "Epoch 93/400\n",
      "140973/140973 - 7s - loss: 2.0851 - accuracy: 0.5318\n",
      "Epoch 94/400\n",
      "140973/140973 - 7s - loss: 2.0802 - accuracy: 0.5327\n",
      "Epoch 95/400\n",
      "140973/140973 - 7s - loss: 2.0745 - accuracy: 0.5338\n",
      "Epoch 96/400\n",
      "140973/140973 - 7s - loss: 2.0730 - accuracy: 0.5340\n",
      "Epoch 97/400\n",
      "140973/140973 - 7s - loss: 2.0677 - accuracy: 0.5355\n",
      "Epoch 98/400\n",
      "140973/140973 - 8s - loss: 2.0620 - accuracy: 0.5360\n",
      "Epoch 99/400\n",
      "140973/140973 - 8s - loss: 2.0593 - accuracy: 0.5373\n",
      "Epoch 100/400\n",
      "140973/140973 - 8s - loss: 2.0525 - accuracy: 0.5391\n",
      "Epoch 101/400\n",
      "140973/140973 - 8s - loss: 2.0485 - accuracy: 0.5387\n",
      "Epoch 102/400\n",
      "140973/140973 - 8s - loss: 2.0441 - accuracy: 0.5404\n",
      "Epoch 103/400\n",
      "140973/140973 - 8s - loss: 2.0405 - accuracy: 0.5409\n",
      "Epoch 104/400\n",
      "140973/140973 - 8s - loss: 2.0360 - accuracy: 0.5416\n",
      "Epoch 105/400\n",
      "140973/140973 - 8s - loss: 2.0324 - accuracy: 0.5426\n",
      "Epoch 106/400\n",
      "140973/140973 - 8s - loss: 2.0274 - accuracy: 0.5436\n",
      "Epoch 107/400\n",
      "140973/140973 - 8s - loss: 2.0231 - accuracy: 0.5444\n",
      "Epoch 108/400\n",
      "140973/140973 - 7s - loss: 2.0209 - accuracy: 0.5444\n",
      "Epoch 109/400\n",
      "140973/140973 - 7s - loss: 2.0167 - accuracy: 0.5457\n",
      "Epoch 110/400\n",
      "140973/140973 - 7s - loss: 2.0148 - accuracy: 0.5457\n",
      "Epoch 111/400\n",
      "140973/140973 - 7s - loss: 2.0106 - accuracy: 0.5464\n",
      "Epoch 112/400\n",
      "140973/140973 - 7s - loss: 2.0060 - accuracy: 0.5480\n",
      "Epoch 113/400\n",
      "140973/140973 - 8s - loss: 2.0020 - accuracy: 0.5489\n",
      "Epoch 114/400\n",
      "140973/140973 - 8s - loss: 1.9978 - accuracy: 0.5499\n",
      "Epoch 115/400\n",
      "140973/140973 - 8s - loss: 1.9947 - accuracy: 0.5498\n",
      "Epoch 116/400\n",
      "140973/140973 - 8s - loss: 1.9893 - accuracy: 0.5509\n",
      "Epoch 117/400\n",
      "140973/140973 - 8s - loss: 1.9892 - accuracy: 0.5512\n",
      "Epoch 118/400\n",
      "140973/140973 - 8s - loss: 1.9841 - accuracy: 0.5518\n",
      "Epoch 119/400\n",
      "140973/140973 - 8s - loss: 1.9799 - accuracy: 0.5526\n",
      "Epoch 120/400\n",
      "140973/140973 - 8s - loss: 1.9775 - accuracy: 0.5527\n",
      "Epoch 121/400\n",
      "140973/140973 - 8s - loss: 1.9758 - accuracy: 0.5541\n",
      "Epoch 122/400\n",
      "140973/140973 - 8s - loss: 1.9735 - accuracy: 0.5541\n",
      "Epoch 123/400\n",
      "140973/140973 - 7s - loss: 1.9685 - accuracy: 0.5556\n",
      "Epoch 124/400\n",
      "140973/140973 - 7s - loss: 1.9661 - accuracy: 0.5560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/400\n",
      "140973/140973 - 7s - loss: 1.9615 - accuracy: 0.5568\n",
      "Epoch 126/400\n",
      "140973/140973 - 7s - loss: 1.9585 - accuracy: 0.5567\n",
      "Epoch 127/400\n",
      "140973/140973 - 8s - loss: 1.9567 - accuracy: 0.5578\n",
      "Epoch 128/400\n",
      "140973/140973 - 8s - loss: 1.9514 - accuracy: 0.5586\n",
      "Epoch 129/400\n",
      "140973/140973 - 8s - loss: 1.9489 - accuracy: 0.5592\n",
      "Epoch 130/400\n",
      "140973/140973 - 8s - loss: 1.9488 - accuracy: 0.5596\n",
      "Epoch 131/400\n",
      "140973/140973 - 8s - loss: 1.9451 - accuracy: 0.5602\n",
      "Epoch 132/400\n",
      "140973/140973 - 8s - loss: 1.9432 - accuracy: 0.5604\n",
      "Epoch 133/400\n",
      "140973/140973 - 8s - loss: 1.9391 - accuracy: 0.5611\n",
      "Epoch 134/400\n",
      "140973/140973 - 8s - loss: 1.9355 - accuracy: 0.5622\n",
      "Epoch 135/400\n",
      "140973/140973 - 8s - loss: 1.9335 - accuracy: 0.5622\n",
      "Epoch 136/400\n",
      "140973/140973 - 8s - loss: 1.9311 - accuracy: 0.5626\n",
      "Epoch 137/400\n",
      "140973/140973 - 8s - loss: 1.9266 - accuracy: 0.5634\n",
      "Epoch 138/400\n",
      "140973/140973 - 8s - loss: 1.9254 - accuracy: 0.5644\n",
      "Epoch 139/400\n",
      "140973/140973 - 7s - loss: 1.9262 - accuracy: 0.5629\n",
      "Epoch 140/400\n",
      "140973/140973 - 7s - loss: 1.9182 - accuracy: 0.5652\n",
      "Epoch 141/400\n",
      "140973/140973 - 7s - loss: 1.9155 - accuracy: 0.5667\n",
      "Epoch 142/400\n",
      "140973/140973 - 7s - loss: 1.9123 - accuracy: 0.5670\n",
      "Epoch 143/400\n",
      "140973/140973 - 8s - loss: 1.9129 - accuracy: 0.5667\n",
      "Epoch 144/400\n",
      "140973/140973 - 8s - loss: 1.9091 - accuracy: 0.5666\n",
      "Epoch 145/400\n",
      "140973/140973 - 8s - loss: 1.9064 - accuracy: 0.5682\n",
      "Epoch 146/400\n",
      "140973/140973 - 8s - loss: 1.9029 - accuracy: 0.5683\n",
      "Epoch 147/400\n",
      "140973/140973 - 8s - loss: 1.9029 - accuracy: 0.5677\n",
      "Epoch 148/400\n",
      "140973/140973 - 8s - loss: 1.9015 - accuracy: 0.5684\n",
      "Epoch 149/400\n",
      "140973/140973 - 8s - loss: 1.8971 - accuracy: 0.5691\n",
      "Epoch 150/400\n",
      "140973/140973 - 8s - loss: 1.8976 - accuracy: 0.5696\n",
      "Epoch 151/400\n",
      "140973/140973 - 8s - loss: 1.8978 - accuracy: 0.5697\n",
      "Epoch 152/400\n",
      "140973/140973 - 7s - loss: 1.8922 - accuracy: 0.5695\n",
      "Epoch 153/400\n",
      "140973/140973 - 7s - loss: 1.8893 - accuracy: 0.5710\n",
      "Epoch 154/400\n",
      "140973/140973 - 7s - loss: 1.8894 - accuracy: 0.5711\n",
      "Epoch 155/400\n",
      "140973/140973 - 7s - loss: 1.8831 - accuracy: 0.5717\n",
      "Epoch 156/400\n",
      "140973/140973 - 8s - loss: 1.8833 - accuracy: 0.5725\n",
      "Epoch 157/400\n",
      "140973/140973 - 8s - loss: 1.8766 - accuracy: 0.5739\n",
      "Epoch 158/400\n",
      "140973/140973 - 8s - loss: 1.8794 - accuracy: 0.5726\n",
      "Epoch 159/400\n",
      "140973/140973 - 8s - loss: 1.8769 - accuracy: 0.5740\n",
      "Epoch 160/400\n",
      "140973/140973 - 8s - loss: 1.8788 - accuracy: 0.5719\n",
      "Epoch 161/400\n",
      "140973/140973 - 8s - loss: 1.8735 - accuracy: 0.5739\n",
      "Epoch 162/400\n",
      "140973/140973 - 8s - loss: 1.8685 - accuracy: 0.5748\n",
      "Epoch 163/400\n",
      "140973/140973 - 8s - loss: 1.8699 - accuracy: 0.5745\n",
      "Epoch 164/400\n",
      "140973/140973 - 8s - loss: 1.8666 - accuracy: 0.5753\n",
      "Epoch 165/400\n",
      "140973/140973 - 8s - loss: 1.8649 - accuracy: 0.5751\n",
      "Epoch 166/400\n",
      "140973/140973 - 7s - loss: 1.8642 - accuracy: 0.5761\n",
      "Epoch 167/400\n",
      "140973/140973 - 7s - loss: 1.8614 - accuracy: 0.5766\n",
      "Epoch 168/400\n",
      "140973/140973 - 7s - loss: 1.8621 - accuracy: 0.5757\n",
      "Epoch 169/400\n",
      "140973/140973 - 8s - loss: 1.8586 - accuracy: 0.5771\n",
      "Epoch 170/400\n",
      "140973/140973 - 8s - loss: 1.8548 - accuracy: 0.5771\n",
      "Epoch 171/400\n",
      "140973/140973 - 8s - loss: 1.8524 - accuracy: 0.5779\n",
      "Epoch 172/400\n",
      "140973/140973 - 8s - loss: 1.8520 - accuracy: 0.5780\n",
      "Epoch 173/400\n",
      "140973/140973 - 8s - loss: 1.8494 - accuracy: 0.5794\n",
      "Epoch 174/400\n",
      "140973/140973 - 8s - loss: 1.8449 - accuracy: 0.5798\n",
      "Epoch 175/400\n",
      "140973/140973 - 8s - loss: 1.8502 - accuracy: 0.5776\n",
      "Epoch 176/400\n",
      "140973/140973 - 8s - loss: 1.8520 - accuracy: 0.5779\n",
      "Epoch 177/400\n",
      "140973/140973 - 8s - loss: 1.8467 - accuracy: 0.5791\n",
      "Epoch 178/400\n",
      "140973/140973 - 8s - loss: 1.8419 - accuracy: 0.5800\n",
      "Epoch 179/400\n",
      "140973/140973 - 8s - loss: 1.8373 - accuracy: 0.5817\n",
      "Epoch 180/400\n",
      "140973/140973 - 7s - loss: 1.8422 - accuracy: 0.5801\n",
      "Epoch 181/400\n",
      "140973/140973 - 7s - loss: 1.8388 - accuracy: 0.5804\n",
      "Epoch 182/400\n",
      "140973/140973 - 7s - loss: 1.8350 - accuracy: 0.5817\n",
      "Epoch 183/400\n",
      "140973/140973 - 8s - loss: 1.8312 - accuracy: 0.5821\n",
      "Epoch 184/400\n",
      "140973/140973 - 8s - loss: 1.8314 - accuracy: 0.5825\n",
      "Epoch 185/400\n",
      "140973/140973 - 8s - loss: 1.8309 - accuracy: 0.5818\n",
      "Epoch 186/400\n",
      "140973/140973 - 8s - loss: 1.8298 - accuracy: 0.5817\n",
      "Epoch 187/400\n",
      "140973/140973 - 8s - loss: 1.8267 - accuracy: 0.5833\n",
      "Epoch 188/400\n",
      "140973/140973 - 8s - loss: 1.8305 - accuracy: 0.5817\n",
      "Epoch 189/400\n",
      "140973/140973 - 8s - loss: 1.8265 - accuracy: 0.5830\n",
      "Epoch 190/400\n",
      "140973/140973 - 8s - loss: 1.8192 - accuracy: 0.5854\n",
      "Epoch 191/400\n",
      "140973/140973 - 8s - loss: 1.8216 - accuracy: 0.5844\n",
      "Epoch 192/400\n",
      "140973/140973 - 8s - loss: 1.8172 - accuracy: 0.5852\n",
      "Epoch 193/400\n",
      "140973/140973 - 7s - loss: 1.8233 - accuracy: 0.5832\n",
      "Epoch 194/400\n",
      "140973/140973 - 7s - loss: 1.8218 - accuracy: 0.5834\n",
      "Epoch 195/400\n",
      "140973/140973 - 7s - loss: 1.8177 - accuracy: 0.5850\n",
      "Epoch 196/400\n",
      "140973/140973 - 7s - loss: 1.8100 - accuracy: 0.5861\n",
      "Epoch 197/400\n",
      "140973/140973 - 7s - loss: 1.8150 - accuracy: 0.5859\n",
      "Epoch 198/400\n",
      "140973/140973 - 8s - loss: 1.8167 - accuracy: 0.5850\n",
      "Epoch 199/400\n",
      "140973/140973 - 8s - loss: 1.8117 - accuracy: 0.5857\n",
      "Epoch 200/400\n",
      "140973/140973 - 8s - loss: 1.8095 - accuracy: 0.5872\n",
      "Epoch 201/400\n",
      "140973/140973 - 8s - loss: 1.8083 - accuracy: 0.5868\n",
      "Epoch 202/400\n",
      "140973/140973 - 8s - loss: 1.8073 - accuracy: 0.5867\n",
      "Epoch 203/400\n",
      "140973/140973 - 8s - loss: 1.8063 - accuracy: 0.5862\n",
      "Epoch 204/400\n",
      "140973/140973 - 8s - loss: 1.8043 - accuracy: 0.5886\n",
      "Epoch 205/400\n",
      "140973/140973 - 8s - loss: 1.8124 - accuracy: 0.5856\n",
      "Epoch 206/400\n",
      "140973/140973 - 8s - loss: 1.8053 - accuracy: 0.5876\n",
      "Epoch 207/400\n",
      "140973/140973 - 8s - loss: 1.8019 - accuracy: 0.5877\n",
      "Epoch 208/400\n",
      "140973/140973 - 7s - loss: 1.8020 - accuracy: 0.5877\n",
      "Epoch 209/400\n",
      "140973/140973 - 7s - loss: 1.7989 - accuracy: 0.5881\n",
      "Epoch 210/400\n",
      "140973/140973 - 7s - loss: 1.7984 - accuracy: 0.5886\n",
      "Epoch 211/400\n",
      "140973/140973 - 7s - loss: 1.7950 - accuracy: 0.5896\n",
      "Epoch 212/400\n",
      "140973/140973 - 8s - loss: 1.7979 - accuracy: 0.5890\n",
      "Epoch 213/400\n",
      "140973/140973 - 8s - loss: 1.7937 - accuracy: 0.5897\n",
      "Epoch 214/400\n",
      "140973/140973 - 8s - loss: 1.7942 - accuracy: 0.5893\n",
      "Epoch 215/400\n",
      "140973/140973 - 8s - loss: 1.7910 - accuracy: 0.5896\n",
      "Epoch 216/400\n",
      "140973/140973 - 8s - loss: 1.7922 - accuracy: 0.5890\n",
      "Epoch 217/400\n",
      "140973/140973 - 8s - loss: 1.7928 - accuracy: 0.5889\n",
      "Epoch 218/400\n",
      "140973/140973 - 8s - loss: 1.7867 - accuracy: 0.5904\n",
      "Epoch 219/400\n",
      "140973/140973 - 8s - loss: 1.7858 - accuracy: 0.5912\n",
      "Epoch 220/400\n",
      "140973/140973 - 8s - loss: 1.7902 - accuracy: 0.5903\n",
      "Epoch 221/400\n",
      "140973/140973 - 8s - loss: 1.7864 - accuracy: 0.5901\n",
      "Epoch 222/400\n",
      "140973/140973 - 8s - loss: 1.7853 - accuracy: 0.5910\n",
      "Epoch 223/400\n",
      "140973/140973 - 7s - loss: 1.7926 - accuracy: 0.5890\n",
      "Epoch 224/400\n",
      "140973/140973 - 7s - loss: 1.7788 - accuracy: 0.5925\n",
      "Epoch 225/400\n",
      "140973/140973 - 7s - loss: 1.7801 - accuracy: 0.5925\n",
      "Epoch 226/400\n",
      "140973/140973 - 7s - loss: 1.7810 - accuracy: 0.5924\n",
      "Epoch 227/400\n",
      "140973/140973 - 8s - loss: 1.7780 - accuracy: 0.5929\n",
      "Epoch 228/400\n",
      "140973/140973 - 8s - loss: 1.7771 - accuracy: 0.5927\n",
      "Epoch 229/400\n",
      "140973/140973 - 8s - loss: 1.7771 - accuracy: 0.5933\n",
      "Epoch 230/400\n",
      "140973/140973 - 8s - loss: 1.7791 - accuracy: 0.5920\n",
      "Epoch 231/400\n",
      "140973/140973 - 8s - loss: 1.7807 - accuracy: 0.5901\n",
      "Epoch 232/400\n",
      "140973/140973 - 8s - loss: 1.7754 - accuracy: 0.5933\n",
      "Epoch 233/400\n",
      "140973/140973 - 8s - loss: 1.7695 - accuracy: 0.5940\n",
      "Epoch 234/400\n",
      "140973/140973 - 8s - loss: 1.7721 - accuracy: 0.5932\n",
      "Epoch 235/400\n",
      "140973/140973 - 8s - loss: 1.7770 - accuracy: 0.5929\n",
      "Epoch 236/400\n",
      "140973/140973 - 8s - loss: 1.7816 - accuracy: 0.5914\n",
      "Epoch 237/400\n",
      "140973/140973 - 7s - loss: 1.7706 - accuracy: 0.5939\n",
      "Epoch 238/400\n",
      "140973/140973 - 7s - loss: 1.7684 - accuracy: 0.5954\n",
      "Epoch 239/400\n",
      "140973/140973 - 7s - loss: 1.7711 - accuracy: 0.5932\n",
      "Epoch 240/400\n",
      "140973/140973 - 7s - loss: 1.7687 - accuracy: 0.5944\n",
      "Epoch 241/400\n",
      "140973/140973 - 7s - loss: 1.7754 - accuracy: 0.5928\n",
      "Epoch 242/400\n",
      "140973/140973 - 8s - loss: 1.7683 - accuracy: 0.5942\n",
      "Epoch 243/400\n",
      "140973/140973 - 8s - loss: 1.7686 - accuracy: 0.5938\n",
      "Epoch 244/400\n",
      "140973/140973 - 8s - loss: 1.7707 - accuracy: 0.5934\n",
      "Epoch 245/400\n",
      "140973/140973 - 8s - loss: 1.7664 - accuracy: 0.5949\n",
      "Epoch 246/400\n",
      "140973/140973 - 8s - loss: 1.7612 - accuracy: 0.5953\n",
      "Epoch 247/400\n",
      "140973/140973 - 8s - loss: 1.7604 - accuracy: 0.5961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/400\n",
      "140973/140973 - 8s - loss: 1.7597 - accuracy: 0.5957\n",
      "Epoch 249/400\n",
      "140973/140973 - 8s - loss: 1.7609 - accuracy: 0.5956\n",
      "Epoch 250/400\n",
      "140973/140973 - 8s - loss: 1.7601 - accuracy: 0.5952\n",
      "Epoch 251/400\n",
      "140973/140973 - 8s - loss: 1.7653 - accuracy: 0.5938\n",
      "Epoch 252/400\n",
      "140973/140973 - 8s - loss: 1.7587 - accuracy: 0.5953\n",
      "Epoch 253/400\n",
      "140973/140973 - 7s - loss: 1.7608 - accuracy: 0.5945\n",
      "Epoch 254/400\n",
      "140973/140973 - 7s - loss: 1.7564 - accuracy: 0.5959\n",
      "Epoch 255/400\n",
      "140973/140973 - 7s - loss: 1.7542 - accuracy: 0.5974\n",
      "Epoch 256/400\n",
      "140973/140973 - 7s - loss: 1.7536 - accuracy: 0.5972\n",
      "Epoch 257/400\n",
      "140973/140973 - 8s - loss: 1.7508 - accuracy: 0.5979\n",
      "Epoch 258/400\n",
      "140973/140973 - 8s - loss: 1.7559 - accuracy: 0.5957\n",
      "Epoch 259/400\n",
      "140973/140973 - 8s - loss: 1.7545 - accuracy: 0.5971\n",
      "Epoch 260/400\n",
      "140973/140973 - 8s - loss: 1.7588 - accuracy: 0.5944\n",
      "Epoch 261/400\n",
      "140973/140973 - 8s - loss: 1.7581 - accuracy: 0.5957\n",
      "Epoch 262/400\n",
      "140973/140973 - 8s - loss: 1.7477 - accuracy: 0.5981\n",
      "Epoch 263/400\n",
      "140973/140973 - 8s - loss: 1.7486 - accuracy: 0.5983\n",
      "Epoch 264/400\n",
      "140973/140973 - 8s - loss: 1.7445 - accuracy: 0.5990\n",
      "Epoch 265/400\n",
      "140973/140973 - 8s - loss: 1.7460 - accuracy: 0.5988\n",
      "Epoch 266/400\n",
      "140973/140973 - 8s - loss: 1.7462 - accuracy: 0.5976\n",
      "Epoch 267/400\n",
      "140973/140973 - 7s - loss: 1.7463 - accuracy: 0.5977\n",
      "Epoch 268/400\n",
      "140973/140973 - 7s - loss: 1.7458 - accuracy: 0.5986\n",
      "Epoch 269/400\n",
      "140973/140973 - 7s - loss: 1.7456 - accuracy: 0.5981\n",
      "Epoch 270/400\n",
      "140973/140973 - 7s - loss: 1.7449 - accuracy: 0.5982\n",
      "Epoch 271/400\n",
      "140973/140973 - 7s - loss: 1.7492 - accuracy: 0.5973\n",
      "Epoch 272/400\n",
      "140973/140973 - 7s - loss: 1.7444 - accuracy: 0.5981\n",
      "Epoch 273/400\n",
      "140973/140973 - 8s - loss: 1.7434 - accuracy: 0.5984\n",
      "Epoch 274/400\n",
      "140973/140973 - 8s - loss: 1.7427 - accuracy: 0.5980\n",
      "Epoch 275/400\n",
      "140973/140973 - 8s - loss: 1.7424 - accuracy: 0.5987\n",
      "Epoch 276/400\n",
      "140973/140973 - 8s - loss: 1.7400 - accuracy: 0.5997\n",
      "Epoch 277/400\n",
      "140973/140973 - 8s - loss: 1.7404 - accuracy: 0.5992\n",
      "Epoch 278/400\n",
      "140973/140973 - 8s - loss: 1.7370 - accuracy: 0.6004\n",
      "Epoch 279/400\n",
      "140973/140973 - 8s - loss: 1.7379 - accuracy: 0.6000\n",
      "Epoch 280/400\n",
      "140973/140973 - 8s - loss: 1.7405 - accuracy: 0.6003\n",
      "Epoch 281/400\n",
      "140973/140973 - 8s - loss: 1.7373 - accuracy: 0.6002\n",
      "Epoch 282/400\n",
      "140973/140973 - 8s - loss: 1.7356 - accuracy: 0.5999\n",
      "Epoch 283/400\n",
      "140973/140973 - 7s - loss: 1.7383 - accuracy: 0.5996\n",
      "Epoch 284/400\n",
      "140973/140973 - 7s - loss: 1.7430 - accuracy: 0.5980\n",
      "Epoch 285/400\n",
      "140973/140973 - 7s - loss: 1.7422 - accuracy: 0.5988\n",
      "Epoch 286/400\n",
      "140973/140973 - 7s - loss: 1.7326 - accuracy: 0.6003\n",
      "Epoch 287/400\n",
      "140973/140973 - 8s - loss: 1.7281 - accuracy: 0.6016\n",
      "Epoch 288/400\n",
      "140973/140973 - 8s - loss: 1.7392 - accuracy: 0.5996\n",
      "Epoch 289/400\n",
      "140973/140973 - 8s - loss: 1.7334 - accuracy: 0.6011\n",
      "Epoch 290/400\n",
      "140973/140973 - 8s - loss: 1.7359 - accuracy: 0.5995\n",
      "Epoch 291/400\n",
      "140973/140973 - 8s - loss: 1.7292 - accuracy: 0.6002\n",
      "Epoch 292/400\n",
      "140973/140973 - 8s - loss: 1.7326 - accuracy: 0.6013\n",
      "Epoch 293/400\n",
      "140973/140973 - 8s - loss: 1.7260 - accuracy: 0.6021\n",
      "Epoch 294/400\n",
      "140973/140973 - 8s - loss: 1.7302 - accuracy: 0.6006\n",
      "Epoch 295/400\n",
      "140973/140973 - 8s - loss: 1.7299 - accuracy: 0.6011\n",
      "Epoch 296/400\n",
      "140973/140973 - 7s - loss: 1.7307 - accuracy: 0.6008\n",
      "Epoch 297/400\n",
      "140973/140973 - 7s - loss: 1.7300 - accuracy: 0.6012\n",
      "Epoch 298/400\n",
      "140973/140973 - 7s - loss: 1.7329 - accuracy: 0.6009\n",
      "Epoch 299/400\n",
      "140973/140973 - 8s - loss: 1.7275 - accuracy: 0.6016\n",
      "Epoch 300/400\n",
      "140973/140973 - 8s - loss: 1.7232 - accuracy: 0.6026\n",
      "Epoch 301/400\n",
      "140973/140973 - 8s - loss: 1.7214 - accuracy: 0.6034\n",
      "Epoch 302/400\n",
      "140973/140973 - 8s - loss: 1.7227 - accuracy: 0.6027\n",
      "Epoch 303/400\n",
      "140973/140973 - 8s - loss: 1.7274 - accuracy: 0.6015\n",
      "Epoch 304/400\n",
      "140973/140973 - 8s - loss: 1.7300 - accuracy: 0.6004\n",
      "Epoch 305/400\n",
      "140973/140973 - 10s - loss: 1.7221 - accuracy: 0.6033\n",
      "Epoch 306/400\n",
      "140973/140973 - 8s - loss: 1.7195 - accuracy: 0.6025\n",
      "Epoch 307/400\n",
      "140973/140973 - 8s - loss: 1.7231 - accuracy: 0.6028\n",
      "Epoch 308/400\n",
      "140973/140973 - 7s - loss: 1.7278 - accuracy: 0.6011\n",
      "Epoch 309/400\n",
      "140973/140973 - 7s - loss: 1.7220 - accuracy: 0.6024\n",
      "Epoch 310/400\n",
      "140973/140973 - 7s - loss: 1.7186 - accuracy: 0.6031\n",
      "Epoch 311/400\n",
      "140973/140973 - 8s - loss: 1.7163 - accuracy: 0.6043\n",
      "Epoch 312/400\n",
      "140973/140973 - 10s - loss: 1.7184 - accuracy: 0.6038\n",
      "Epoch 313/400\n",
      "140973/140973 - 8s - loss: 1.7204 - accuracy: 0.6037\n",
      "Epoch 314/400\n",
      "140973/140973 - 8s - loss: 1.7256 - accuracy: 0.6012\n",
      "Epoch 315/400\n",
      "140973/140973 - 8s - loss: 1.7191 - accuracy: 0.6027\n",
      "Epoch 316/400\n",
      "140973/140973 - 8s - loss: 1.7176 - accuracy: 0.6035\n",
      "Epoch 317/400\n",
      "140973/140973 - 8s - loss: 1.7166 - accuracy: 0.6042\n",
      "Epoch 318/400\n",
      "140973/140973 - 8s - loss: 1.7236 - accuracy: 0.6032\n",
      "Epoch 319/400\n",
      "140973/140973 - 8s - loss: 1.7235 - accuracy: 0.6018\n",
      "Epoch 320/400\n",
      "140973/140973 - 8s - loss: 1.7173 - accuracy: 0.6030\n",
      "Epoch 321/400\n",
      "140973/140973 - 8s - loss: 1.7117 - accuracy: 0.6040\n",
      "Epoch 322/400\n",
      "140973/140973 - 8s - loss: 1.7170 - accuracy: 0.6030\n",
      "Epoch 323/400\n",
      "140973/140973 - 8s - loss: 1.7192 - accuracy: 0.6030\n",
      "Epoch 324/400\n",
      "140973/140973 - 8s - loss: 1.7195 - accuracy: 0.6034\n",
      "Epoch 325/400\n",
      "140973/140973 - 7s - loss: 1.7191 - accuracy: 0.6032\n",
      "Epoch 326/400\n",
      "140973/140973 - 7s - loss: 1.7162 - accuracy: 0.6039\n",
      "Epoch 327/400\n",
      "140973/140973 - 7s - loss: 1.7104 - accuracy: 0.6052\n",
      "Epoch 328/400\n",
      "140973/140973 - 8s - loss: 1.7096 - accuracy: 0.6053\n",
      "Epoch 329/400\n",
      "140973/140973 - 8s - loss: 1.7125 - accuracy: 0.6050\n",
      "Epoch 330/400\n",
      "140973/140973 - 8s - loss: 1.7149 - accuracy: 0.6028\n",
      "Epoch 331/400\n",
      "140973/140973 - 8s - loss: 1.7135 - accuracy: 0.6040\n",
      "Epoch 332/400\n",
      "140973/140973 - 8s - loss: 1.7083 - accuracy: 0.6055\n",
      "Epoch 333/400\n",
      "140973/140973 - 8s - loss: 1.7125 - accuracy: 0.6039\n",
      "Epoch 334/400\n",
      "140973/140973 - 8s - loss: 1.7107 - accuracy: 0.6039\n",
      "Epoch 335/400\n",
      "140973/140973 - 8s - loss: 1.7061 - accuracy: 0.6064\n",
      "Epoch 336/400\n",
      "140973/140973 - 8s - loss: 1.7053 - accuracy: 0.6049\n",
      "Epoch 337/400\n",
      "140973/140973 - 8s - loss: 1.7086 - accuracy: 0.6058\n",
      "Epoch 338/400\n",
      "140973/140973 - 8s - loss: 1.7036 - accuracy: 0.6062\n",
      "Epoch 339/400\n",
      "140973/140973 - 8s - loss: 1.7080 - accuracy: 0.6048\n",
      "Epoch 340/400\n",
      "140973/140973 - 8s - loss: 1.7265 - accuracy: 0.6015\n",
      "Epoch 341/400\n",
      "140973/140973 - 8s - loss: 1.7183 - accuracy: 0.6030\n",
      "Epoch 342/400\n",
      "140973/140973 - 8s - loss: 1.7044 - accuracy: 0.6070\n",
      "Epoch 343/400\n",
      "140973/140973 - 8s - loss: 1.6981 - accuracy: 0.6078\n",
      "Epoch 344/400\n",
      "140973/140973 - 8s - loss: 1.7048 - accuracy: 0.6060\n",
      "Epoch 345/400\n",
      "140973/140973 - 9s - loss: 1.7104 - accuracy: 0.6036\n",
      "Epoch 346/400\n",
      "140973/140973 - 9s - loss: 1.7044 - accuracy: 0.6065\n",
      "Epoch 347/400\n",
      "140973/140973 - 8s - loss: 1.6991 - accuracy: 0.6061\n",
      "Epoch 348/400\n",
      "140973/140973 - 8s - loss: 1.7011 - accuracy: 0.6074\n",
      "Epoch 349/400\n",
      "140973/140973 - 8s - loss: 1.7030 - accuracy: 0.6070\n",
      "Epoch 350/400\n",
      "140973/140973 - 8s - loss: 1.6978 - accuracy: 0.6079\n",
      "Epoch 351/400\n",
      "140973/140973 - 8s - loss: 1.7043 - accuracy: 0.6058\n",
      "Epoch 352/400\n",
      "140973/140973 - 8s - loss: 1.7021 - accuracy: 0.6062\n",
      "Epoch 353/400\n",
      "140973/140973 - 8s - loss: 1.7115 - accuracy: 0.6038\n",
      "Epoch 354/400\n",
      "140973/140973 - 8s - loss: 1.7027 - accuracy: 0.6053\n",
      "Epoch 355/400\n",
      "140973/140973 - 8s - loss: 1.6990 - accuracy: 0.6072\n",
      "Epoch 356/400\n",
      "140973/140973 - 8s - loss: 1.7040 - accuracy: 0.6052\n",
      "Epoch 357/400\n",
      "140973/140973 - 8s - loss: 1.6978 - accuracy: 0.6077\n",
      "Epoch 358/400\n",
      "140973/140973 - 8s - loss: 1.6944 - accuracy: 0.6084\n",
      "Epoch 359/400\n",
      "140973/140973 - 8s - loss: 1.6992 - accuracy: 0.6061\n",
      "Epoch 360/400\n",
      "140973/140973 - 8s - loss: 1.7022 - accuracy: 0.6058\n",
      "Epoch 361/400\n",
      "140973/140973 - 8s - loss: 1.7019 - accuracy: 0.6054\n",
      "Epoch 362/400\n",
      "140973/140973 - 8s - loss: 1.7015 - accuracy: 0.6053\n",
      "Epoch 363/400\n",
      "140973/140973 - 8s - loss: 1.7002 - accuracy: 0.6067\n",
      "Epoch 364/400\n",
      "140973/140973 - 8s - loss: 1.6964 - accuracy: 0.6071\n",
      "Epoch 365/400\n",
      "140973/140973 - 8s - loss: 1.6935 - accuracy: 0.6091\n",
      "Epoch 366/400\n",
      "140973/140973 - 8s - loss: 1.7026 - accuracy: 0.6062\n",
      "Epoch 367/400\n",
      "140973/140973 - 8s - loss: 1.6991 - accuracy: 0.6061\n",
      "Epoch 368/400\n",
      "140973/140973 - 8s - loss: 1.6991 - accuracy: 0.6063\n",
      "Epoch 369/400\n",
      "140973/140973 - 8s - loss: 1.7037 - accuracy: 0.6050\n",
      "Epoch 370/400\n",
      "140973/140973 - 8s - loss: 1.7008 - accuracy: 0.6067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/400\n",
      "140973/140973 - 8s - loss: 1.6966 - accuracy: 0.6076\n",
      "Epoch 372/400\n",
      "140973/140973 - 8s - loss: 1.6974 - accuracy: 0.6069\n",
      "Epoch 373/400\n",
      "140973/140973 - 8s - loss: 1.6974 - accuracy: 0.6068\n",
      "Epoch 374/400\n",
      "140973/140973 - 8s - loss: 1.6958 - accuracy: 0.6069\n",
      "Epoch 375/400\n",
      "140973/140973 - 8s - loss: 1.7034 - accuracy: 0.6055\n",
      "Epoch 376/400\n",
      "140973/140973 - 8s - loss: 1.7059 - accuracy: 0.6045\n",
      "Epoch 377/400\n",
      "140973/140973 - 8s - loss: 1.6912 - accuracy: 0.6092\n",
      "Epoch 378/400\n",
      "140973/140973 - 8s - loss: 1.6954 - accuracy: 0.6075\n",
      "Epoch 379/400\n",
      "140973/140973 - 8s - loss: 1.6896 - accuracy: 0.6082\n",
      "Epoch 380/400\n",
      "140973/140973 - 7s - loss: 1.6991 - accuracy: 0.6064\n",
      "Epoch 381/400\n",
      "140973/140973 - 8s - loss: 1.6920 - accuracy: 0.6084\n",
      "Epoch 382/400\n",
      "140973/140973 - 8s - loss: 1.6918 - accuracy: 0.6084\n",
      "Epoch 383/400\n",
      "140973/140973 - 8s - loss: 1.6903 - accuracy: 0.6081\n",
      "Epoch 384/400\n",
      "140973/140973 - 8s - loss: 1.7032 - accuracy: 0.6045\n",
      "Epoch 385/400\n",
      "140973/140973 - 8s - loss: 1.6944 - accuracy: 0.6080\n",
      "Epoch 386/400\n",
      "140973/140973 - 9s - loss: 1.6878 - accuracy: 0.6089\n",
      "Epoch 387/400\n",
      "140973/140973 - 8s - loss: 1.6890 - accuracy: 0.6088\n",
      "Epoch 388/400\n",
      "140973/140973 - 8s - loss: 1.6912 - accuracy: 0.6075\n",
      "Epoch 389/400\n",
      "140973/140973 - 8s - loss: 1.7002 - accuracy: 0.6062\n",
      "Epoch 390/400\n",
      "140973/140973 - 8s - loss: 1.6886 - accuracy: 0.6086\n",
      "Epoch 391/400\n",
      "140973/140973 - 8s - loss: 1.7022 - accuracy: 0.6050\n",
      "Epoch 392/400\n",
      "140973/140973 - 8s - loss: 1.6888 - accuracy: 0.6087\n",
      "Epoch 393/400\n",
      "140973/140973 - 8s - loss: 1.6812 - accuracy: 0.6107\n",
      "Epoch 394/400\n",
      "140973/140973 - 8s - loss: 1.6877 - accuracy: 0.6091\n",
      "Epoch 395/400\n",
      "140973/140973 - 8s - loss: 1.6842 - accuracy: 0.6098\n",
      "Epoch 396/400\n",
      "140973/140973 - 8s - loss: 1.6889 - accuracy: 0.6089\n",
      "Epoch 397/400\n",
      "140973/140973 - 9s - loss: 1.6917 - accuracy: 0.6073\n",
      "Epoch 398/400\n",
      "140973/140973 - 8s - loss: 1.6850 - accuracy: 0.6107\n",
      "Epoch 399/400\n",
      "140973/140973 - 8s - loss: 1.6855 - accuracy: 0.6101\n",
      "Epoch 400/400\n",
      "140973/140973 - 8s - loss: 1.6956 - accuracy: 0.6062\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.time() \n",
    "\n",
    "history = model.fit(data_X, data_Y,\n",
    "          batch_size=256,\n",
    "          epochs=400, \n",
    "          verbose=2)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Time: 3073.936546564102'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Time: {}\".format(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from nltk.translate import bleu_score\n",
    "\n",
    "def text_generation(model):\n",
    "    text_idx = 0\n",
    "    current_text = text[text_idx]\n",
    "    generated = ''\n",
    "    sentence = current_text[:maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(200): \n",
    "        x_pred = list()\n",
    "        for idx, char in enumerate(sentence): \n",
    "            x_pred.append([char_indices[char]])\n",
    "\n",
    "        x_pred = np.asarray(x_pred)\n",
    "        preds = model.predict(x_pred.T)[0] \n",
    "        next_idx = np.argmax(preds)\n",
    "        next_char = indices_char[next_idx]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "        if next_char == 'E': break \n",
    "    print()\n",
    "\n",
    "def text_generation_res(model):\n",
    "    text_idx = 0\n",
    "    current_text = text[text_idx]\n",
    "    generated = ''\n",
    "    sentence = current_text[:maxlen]\n",
    "    \n",
    "    for i in range(min(200, len(current_text))): \n",
    "        x_pred = list()\n",
    "        for idx, char in enumerate(sentence): \n",
    "            x_pred.append([char_indices[char]])\n",
    "\n",
    "        x_pred = np.asarray(x_pred)\n",
    "        preds = model.predict(x_pred.T)[0] \n",
    "        next_idx = np.argmax(preds)\n",
    "        next_char = indices_char[next_idx]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        generated += next_char\n",
    "        if next_char == 'E': break\n",
    "        \n",
    "    return generated, text_idx\n",
    "\n",
    "def get_BLEU_score(model): \n",
    "    predict_text, text_idx = text_generation_res(model) \n",
    "    pred_len = len(predict_text) \n",
    "    original_text = text[text_idx] \n",
    "    original_text = original_text[:pred_len]\n",
    "\n",
    "    original_text = original_text.split()\n",
    "    predict_text = predict_text.split() \n",
    "    BLEUscore = bleu_score.sentence_bleu([original_text], predict_text)\n",
    "    return BLEUscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"제임스 얼 \"지미\" 카터\"\n",
      "제임스 얼 \"지미\" 카터 욕구를 지냈다.\n",
      "1967년 맥스웰은 민주당 후보로 인해 10월에는 데이터 문재인 비판일 후보 세계 대변공화학에서 다음과 같다.\n",
      "물리 화합물을 위해 여러 역시 함수의 때문에 위튼 정치 구속 대통령 서·태조사에서 \"대통령 교전 대통령 교전 조직의 이 전시와 민주당 후보 세계 수집에서는 19세기 소분을 이룬다. 그러나 한미 ftac 안상 사건을 합은 찢어버리,\n"
     ]
    }
   ],
   "source": [
    "text_generation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dwpar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\dwpar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\dwpar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.885877678021066e-232"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_BLEU_score(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "evalutaor = Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=4,\n",
    "                           apply_avg='Avg',\n",
    "                           alpha=0.5,\n",
    "                           weight_factor=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-3': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       " 'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       " 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       " 'rouge-4': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       " 'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       " 'rouge-w': {'f': 0.0, 'p': 0.0, 'r': 0.0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ROUGE_score(model, evaluator): \n",
    "    predict_text, text_idx = text_generation_res(model) \n",
    "    pred_len = len(predict_text) \n",
    "    original_text = text[text_idx] \n",
    "    score = evalutaor.get_scores(original_text, predict_text)\n",
    "    return score\n",
    "get_ROUGE_score(model, evalutaor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
